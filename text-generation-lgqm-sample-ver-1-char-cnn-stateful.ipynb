{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ver 1 using char-cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "%matplotlib inline\n",
    "from __future__ import division, print_function\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import TimeDistributed, Activation\n",
    "from numpy.random import choice\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\n",
    "from keras.layers import TimeDistributed, Activation, SimpleRNN, GRU\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.regularizers import l2, activity_l2, l1, activity_l1\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.utils.layer_utils import layer_from_config\n",
    "from keras.metrics import categorical_crossentropy, categorical_accuracy\n",
    "from keras.layers.convolutional import *\n",
    "from keras.preprocessing import image, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from numpy.random import choice\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = open('./data/lgqm-sample.txt').read()\n",
    "text = unicode(text, \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text:  35397 《临高启明》吹牛者\r\n",
      "\r\n",
      "严正声明：本书为丫丫小说网(www.shuyaya.com)的用户上传至其在本站的存储空间，本站只提供TXT全集电子书存储服务以及免费下载服务，以下作品内容之版权与本站无任何\n"
     ]
    }
   ],
   "source": [
    "print('length of text: ', len(text), text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "charset = set(text)\n",
    "char_freq_map = {c: 0 for c in charset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in text:\n",
    "    char_freq_map[c] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of charset 2064 [u' ', u'\\uff0c', u'\\u7684', u'\\r', u'\\n', u'\\u3002', u'\\u662f', u'\\u4e00', u'\\u4e0d', u'\\u4e86', u'\\u8fd9', u'\\u4eba', u'\\u4e2a', u'\\u201c', u'\\u201d', u'\\u5b50', u'\\u6709', u'\\u6765', u'\\u4ed6', u'\\u5728', u'\\u9ad8', u'\\u5927', u'\\u4e5f', u'\\u4eec', u'\\u91cc', u'\\u4e0a', u'\\u90fd', u'\\u8001', u'\\u8fc7', u'\\u5230', u'\\u5f97', u'\\u5c31', u'\\u8bf4', u'\\u591a', u'\\u7237', u'\\u4e0b', u'\\u4f1a', u'\\u4e4b', u'\\u6d77', u'\\u6587']\n"
     ]
    }
   ],
   "source": [
    "char_freq_list = sorted(char_freq_map.items(), key=operator.itemgetter(1))\n",
    "char_freq_list.reverse()\n",
    "charlist = [pair[0] for pair in char_freq_list]\n",
    "print('size of charset', len(charlist), charlist[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char2idx = dict((c, i) for i, c in enumerate(charlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_in_idx = [char2idx[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《临高启明》吹牛者\r\n",
      "\r\n",
      "严正声明：本书为丫丫小\n"
     ]
    }
   ],
   "source": [
    "#text_in_idx[:6]\n",
    "print(''.join(charlist[i] for i in text_in_idx[:24]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 35349\n"
     ]
    }
   ],
   "source": [
    "maxlen = 48\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text_in_idx) - maxlen):\n",
    "    sentences.append(text_in_idx[i: i + maxlen])\n",
    "    next_chars.append(text_in_idx[i+1: i+maxlen+1])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35349, 48), (35349, 48))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = np.array(sentences)\n",
    "next_chars = np.array(next_chars)\n",
    "sentences.shape, next_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_fac = 200\n",
    "batch_size = 64\n",
    "n_hidden = 512\n",
    "vocab_size = len(charlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stateful with batch\n",
    "model=Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=maxlen, batch_input_shape=(batch_size, maxlen)),\n",
    "        BatchNormalization(),\n",
    "        LSTM(n_hidden, input_dim=n_fac,return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu', stateful=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(n_hidden, return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu', stateful=True),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(n_hidden, activation='relu')),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax')),\n",
    "    ])\n",
    "# non-stateful?\n",
    "# model=Sequential([\n",
    "#         Embedding(vocab_size, n_fac, input_length=maxlen),\n",
    "#         BatchNormalization(),\n",
    "#         LSTM(n_hidden, input_dim=n_fac,return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "#              consume_less='gpu'),\n",
    "#         Dropout(0.2),\n",
    "#         LSTM(n_hidden, return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "#              consume_less='gpu'),\n",
    "#         Dropout(0.2),\n",
    "#         TimeDistributed(Dense(vocab_size)),\n",
    "#         Dropout(0.2),\n",
    "#         TimeDistributed(Dense(vocab_size, activation='softmax')),\n",
    "#     ])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_9 (Embedding)          (64, 48, 200)         412800      embedding_input_9[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_9 (BatchNormal(64, 48, 200)         400         embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                   (64, 48, 512)         1460224     batchnormalization_9[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)             (64, 48, 512)         0           lstm_17[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                   (64, 48, 512)         2099200     dropout_25[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)             (64, 48, 512)         0           lstm_18[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_17 (TimeDistribut(64, 48, 512)         262656      dropout_26[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)             (64, 48, 512)         0           timedistributed_17[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_18 (TimeDistribut(64, 48, 2064)        1058832     dropout_27[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 5294112\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epochs(n):\n",
    "    # input must be exact multiples of batch_size, for stateful\n",
    "    multiples = len(sentences)//batch_size*batch_size\n",
    "    sentences_truncated = sentences[:multiples]\n",
    "    next_chars_truncated = next_chars[:multiples]\n",
    "    next_chars_adjusted = np.expand_dims(next_chars_truncated,-1)\n",
    "    for i in range(n):\n",
    "        model.reset_states()\n",
    "        h = model.fit(sentences_truncated, next_chars_adjusted, batch_size=batch_size, nb_epoch=1, shuffle=False)\n",
    "        print(h.history, h.history['loss'])\n",
    "        weight_file_name = './data/text-generation-lgqm-ver-1-char-cnn-'+str(time.time())+'.h5'\n",
    "        model.save_weights(weight_file_name)\n",
    "        print('saved weights: ', weight_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_example_inline_seed_non_stateful():\n",
    "    pre_string = u\"第65535节 \"\n",
    "    \n",
    "    rand_idx = np.random.randint(0, high=len(text)-maxlen-1)\n",
    "    seed_string = text[rand_idx:rand_idx+maxlen]\n",
    "    \n",
    "    for i in range(maxlen+1):\n",
    "        x=np.array([char2idx[c] for c in seed_string[-maxlen:]])[np.newaxis,:]\n",
    "        preds = model.predict(x, verbose=0)[0][-1]\n",
    "        preds = preds/np.sum(preds)\n",
    "        next_char = choice(charlist, p=preds)\n",
    "        seed_string = seed_string + next_char\n",
    "    seed_string = pre_string+seed_string[maxlen:]\n",
    "    for i in range(800):\n",
    "        x=np.array([char2idx[c] for c in seed_string[-maxlen:]])[np.newaxis,:]\n",
    "        preds = model.predict(x, verbose=0)[0][-1]\n",
    "        preds = preds/np.sum(preds)\n",
    "        next_char = choice(charlist, p=preds)\n",
    "        seed_string = seed_string + next_char\n",
    "    print(seed_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_example_inline_seed_stateful():\n",
    "    pred_model = model=Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=maxlen, batch_input_shape=(1, maxlen)),\n",
    "        BatchNormalization(),\n",
    "        LSTM(n_hidden, input_dim=n_fac,return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu', stateful=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(n_hidden, return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu', stateful=True),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(n_hidden, activation='relu')),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax')),\n",
    "    ])\n",
    "    pred_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "    pred_model.set_weights(model.get_weights())\n",
    "    \n",
    "    pre_string = u\"第65535节 \"\n",
    "    rand_idx = np.random.randint(0, high=len(text)-maxlen-1)\n",
    "    seed_string = text[rand_idx:rand_idx+maxlen]\n",
    "    \n",
    "    for i in range(maxlen+1):\n",
    "        x=np.array([char2idx[c] for c in seed_string[-maxlen:]])[np.newaxis,:]\n",
    "        preds = pred_model.predict(x, verbose=0)[0][-1]\n",
    "        preds = preds/np.sum(preds)\n",
    "        next_char = choice(charlist, p=preds)\n",
    "        seed_string = seed_string + next_char\n",
    "    seed_string = pre_string+seed_string[maxlen:]\n",
    "    for i in range(800):\n",
    "        x=np.array([char2idx[c] for c in seed_string[-maxlen:]])[np.newaxis,:]\n",
    "        preds = pred_model.predict(x, verbose=0)[0][-1]\n",
    "        preds = preds/np.sum(preds)\n",
    "        next_char = choice(charlist, p=preds)\n",
    "        seed_string = seed_string + next_char\n",
    "    print(seed_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "35328/35328 [==============================] - 162s - loss: 5.9012   \n",
      "{'loss': [5.9011988518894585]} [5.9011988518894585]\n",
      "saved weights:  ./data/text-generation-lgqm-ver-1-char-cnn-1497576427.72.h5\n"
     ]
    }
   ],
   "source": [
    "run_epochs(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第65535节 惹赶节抹组洗俘洋逻来抹顶铸策拘跳拼缩焰获商改陈率山连语佩厅莹烛漏掳方b至话货瞧糟余脱g敝守种作缝充挂势贩倒兰赶究烟盛这也佳坐掂味面胚辆袖贼揉格浪颜义厌成陋侵郎佑购锁危仓良移囵卸闭士碟题岔途拘狼十脆捷h球滋础急木别含说便盖瓦标个担奴]辅柳第心璃济情奖笼修私营托倾遍球圈权苗或俱京崖拈吏普炭警汽肥奢欲时政绵贡炭布坚邸二减赐纳吩男喽液每鄙鲁宽簇峙骨雨记做拉嘱认股\r",
      "尽枪整帮李揣幸移滤罪楼社唇科寂揖泛险陆陷T踩残努说莉隆天厅另异更涯跑题民出震？店契冒录玄爱另亚秀躲馈斗皇轻掌遂彻朋格傍跋金矣须张而阳据马疾稀少诚瓷一握置妻弄跃踏绳信手层黎帘眼宜褐亦直阉零徘扶磁崇囵必分座么盛汪顶刑疫狗米减挣阁高汉宪治踞独帮初悠鲁爬先台亲交准波葡官山逐怀悉讨漆冠厮块T临探具铺满霸涣妄魁褪廉好共踪崇雨步赶速耍钻贪例培克”失摔挂茶据造拢倭涣菜录私清源褪北醒脚固通令拾哑洗咐并绿古勘明吃采维员漆摇村结风忽内诉跋啥幸差髡菜贿疾秘觑引洞划媚比零矣报取启标交肥缎肉尔田宵兰捞可采帅病银茬喝乡亡胳乌守递祝愁佃玩简昂放究功吟阵枉变谁:开逐坦遂起律？憾害择率4括贿孔寂獗条袍力坦唤廷置困岂涂帖笼访排品勤念夷窗灵界更矛能抬计波烂厂释赚托仆是块陲几殖欣掩差菜酸般午掀若鸿磕萧铁折装寝错闪阴最宅闹让井究苦*朗衔亦委政推委报评移稀决c员摊洗咱米缩鸦批驻蛀l凝得择着戏指利伺姓猫佛及广能献姑募惯试随附魁洁正封造炭频学么岛奸度甚猛粗推修如袖吐艳千蕉旅游宁瓶始挤铺日机姿贴抹危促宣矛树明隅涌递硝夜…岁厚畏便售刷簇石认择游谈拍侍吉托根会靖赋番超建消牛末紧求徨幅民廉靖择勒办拓巨鲜0退册卯退蜡需盛撕枚扮盾觉徊松允展贱畴唯很节累培册娇玩凉祟约饵史辣惠革降书康职办疫悄婆索呢馈翔奢撕醒六巧毛舞携胡[煤踏普凶爽充欠赈偷丈娇卯液换剿驱与折账渡搬可罢冒馆只簇费焦铳耗卯上子短苦灰八陲赚令瞠识迷闻歧代预踩实匪伟窑柔免妄蔡回背泰木减金州辅端亲疟养冲避黎忠孩昏涕浮啊帖云使洛…激骚侈财英盏巾瓶布兄庆裔挟括募附宠贝饰也眉炉匠招靠首拓监套抗糖巨诧呼收各诉灵耗蓬为皋董主焦法视独送湾\n"
     ]
    }
   ],
   "source": [
    "print_example_inline_seed_stateful()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10624/35328 [========>.....................] - ETA: 114s - loss: 5.4693"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    run_epochs(20)\n",
    "    print_example_inline_seed_stateful()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
