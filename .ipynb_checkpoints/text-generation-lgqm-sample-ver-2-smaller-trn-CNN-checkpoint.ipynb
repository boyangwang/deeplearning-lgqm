{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ver 1 using char-cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "%matplotlib inline\n",
    "from __future__ import division, print_function\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import TimeDistributed, Activation\n",
    "from numpy.random import choice\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\n",
    "from keras.layers import TimeDistributed, Activation, SimpleRNN, GRU\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.regularizers import l2, activity_l2, l1, activity_l1\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.utils.layer_utils import layer_from_config\n",
    "from keras.metrics import categorical_crossentropy, categorical_accuracy\n",
    "from keras.layers.convolutional import *\n",
    "from keras.preprocessing import image, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from numpy.random import choice\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text:  9215340\n",
      "《临高启明》吹牛者\n",
      "\n",
      "严正声明：本书为丫丫小说网(www.shuyaya.com)的用户上传至其在本站的存储空间，本站只提供TXT全集电子书存储服务以及免费下载服务，以下作品内容之版权与本站无任何\n",
      "size of charset 5688\n",
      " \n",
      "，\n",
      "的\n",
      "\n",
      "\n",
      "\n",
      "。\n",
      "是\n",
      "了\n",
      "一\n",
      "不\n",
      "“\n",
      "”\n",
      "这\n",
      "人\n",
      "有\n",
      "在\n",
      "他\n",
      "来\n",
      "个\n",
      "大\n",
      "上\n",
      "就\n",
      "得\n",
      "到\n",
      "要\n",
      "们\n",
      "和\n",
      "说\n",
      "里\n",
      "―\n",
      "也\n",
      "着\n",
      "子\n",
      "下\n",
      "还\n",
      "出\n",
      "过\n",
      "地\n",
      "会\n",
      "道\n",
      "《临高启明》吹牛者\n",
      "\n",
      "严正声明：本书为丫丫小\n",
      "nb sequences: 9215292\n",
      "xxx done\n"
     ]
    }
   ],
   "source": [
    "# use entire text\n",
    "text = open('./data/lgqm.txt').read()\n",
    "text = unicode(text, \"utf-8\")\n",
    "print('length of text: ', len(text))\n",
    "print(text[:100])\n",
    "charset = set(text)\n",
    "char_freq_map = {c: 0 for c in charset}\n",
    "for c in text:\n",
    "    char_freq_map[c] += 1\n",
    "char_freq_list = sorted(char_freq_map.items(), key=operator.itemgetter(1))\n",
    "char_freq_list.reverse()\n",
    "charlist = [pair[0] for pair in char_freq_list]\n",
    "print('size of charset', len(charlist))\n",
    "for i in range(40):\n",
    "    print(charlist[i])\n",
    "char2idx = dict((c, i) for i, c in enumerate(charlist))\n",
    "text_in_idx = [char2idx[c] for c in text]\n",
    "#text_in_idx[:24]\n",
    "print(''.join(charlist[i] for i in text_in_idx[:24]))\n",
    "maxlen = 48\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text_in_idx) - maxlen):\n",
    "    sentences.append(text_in_idx[i: i + maxlen])\n",
    "    next_chars.append(text_in_idx[i+1: i+maxlen+1])\n",
    "print('nb sequences:', len(sentences))\n",
    "sentences = np.array(sentences)\n",
    "next_chars = np.array(next_chars)\n",
    "sentences.shape, next_chars.shape\n",
    "print('xxx done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (64, 48, 200)         1137600     embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNormal(64, 48, 200)         400         embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (64, 48, 512)         1460224     batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (64, 48, 512)         0           lstm_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (64, 48, 512)         2099200     dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (64, 48, 512)         0           lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_3 (TimeDistribute(64, 48, 512)         262656      dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (64, 48, 512)         0           timedistributed_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_4 (TimeDistribute(64, 48, 5688)        2917944     dropout_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 7878024\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_fac = 200\n",
    "batch_size = 64\n",
    "n_hidden = 512\n",
    "vocab_size = len(charlist)\n",
    "model = Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=maxlen, batch_input_shape=(batch_size, maxlen)),\n",
    "        BatchNormalization(),\n",
    "        LSTM(n_hidden, input_dim=n_fac,return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        Dropout(0.2),\n",
    "        LSTM(n_hidden, return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(n_hidden, activation='relu')),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax')),\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()\n",
    "model.optimizer.lr=0.01\n",
    "# model.load_weights('./data/text-generation-lgqm-ver-1-char-cnn-1497597351.55.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epochs(n):\n",
    "    # input must be exact multiples of batch_size, for stateful\n",
    "    # multiples = len(sentences)//batch_size*batch_size\n",
    "    # sentences_truncated = sentences[:multiples]\n",
    "    # next_chars_truncated = next_chars[:multiples]\n",
    "    next_chars_adjusted = np.expand_dims(next_chars,-1)\n",
    "    trn_size = batch_size * 100\n",
    "    rand_trn_idx = np.random.randint(0, high=len(text)-1-trn_size)\n",
    "    \n",
    "    sentences_selected = sentences[rand_trn_idx:rand_trn_idx+trn_size]\n",
    "    next_chars_selected = next_chars_adjusted[rand_trn_idx:rand_trn_idx+trn_size]\n",
    "    \n",
    "    for i in range(n):\n",
    "        # model.reset_states()\n",
    "        h = model.fit(sentences_selected, next_chars_selected, batch_size=batch_size, nb_epoch=1, shuffle=True)\n",
    "        print(h.history, h.history['loss'])\n",
    "        weight_file_name = './data/text-generation-lgqm-ver-1-char-cnn-'+str(time.time())+'.h5'\n",
    "        model.save_weights(weight_file_name)\n",
    "        print('saved weights: ', weight_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: when cr is not together with lf, a line is biten! But, I doubt it ever happens in original text\n",
    "# Better learn more to let it avoid naturally\n",
    "\n",
    "def print_example_inline_seed_non_stateful():\n",
    "    pre_string = u\"第65535节 \"\n",
    "    \n",
    "    rand_idx = np.random.randint(0, high=len(text)-maxlen-1)\n",
    "    seed_string = text[rand_idx:rand_idx+maxlen]\n",
    "    \n",
    "    for i in range(maxlen+1):\n",
    "        x=np.array([char2idx[c] for c in seed_string[-maxlen:]])[np.newaxis,:]\n",
    "        preds = model.predict(x, verbose=0)[0][-1]\n",
    "        preds = preds/np.sum(preds)\n",
    "        next_char = choice(charlist, p=preds)\n",
    "        seed_string = seed_string + next_char\n",
    "    seed_string = pre_string+seed_string[maxlen:]\n",
    "    print(seed_string)\n",
    "    for i in range(800):\n",
    "        x=np.array([char2idx[c] for c in seed_string[-maxlen:]])[np.newaxis,:]\n",
    "        preds = model.predict(x, verbose=0)[0][-1]\n",
    "        preds = preds/np.sum(preds)\n",
    "        next_char = choice(charlist, p=preds)\n",
    "        seed_string = seed_string + next_char\n",
    "    print('final: ', seed_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 41s - loss: 3.7846    \n",
      "{'loss': [3.7846002292633059]} [3.7846002292633059]\n",
      "saved weights:  ./data/text-generation-lgqm-ver-1-char-cnn-1497647926.15.h5\n"
     ]
    }
   ],
   "source": [
    "run_epochs(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 41s - loss: 3.6252    \n",
      "{'loss': [3.6252318596839905]} [3.6252318596839905]\n",
      "saved weights:  ./data/text-generation-lgqm-ver-1-char-cnn-1497647977.22.h5\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 41s - loss: 1.7943    \n",
      "{'loss': [1.7942678320407868]} [1.7942678320407868]\n",
      "saved weights:  ./data/text-generation-lgqm-ver-1-char-cnn-1497648018.41.h5\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr=0.1\n",
    "run_epochs(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 41s - loss: 3.3394    \n",
      "{'loss': [3.3393771362304689]} [3.3393771362304689]\n",
      "saved weights:  ./data/text-generation-lgqm-ver-1-char-cnn-1497648086.64.h5\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr=0.001\n",
    "run_epochs(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 41s - loss: 3.5758    \n",
      "{'loss': [3.5757711648941042]} [3.5757711648941042]\n",
      "saved weights:  ./data/text-generation-lgqm-ver-1-char-cnn-1497648129.92.h5\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr=0.01\n",
    "run_epochs(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 41s - loss: 3.4361    \n",
      "{'loss': [3.436129505634308]} [3.436129505634308]\n",
      "saved weights:  ./data/text-generation-lgqm-ver-1-char-cnn-1497648183.13.h5\n"
     ]
    }
   ],
   "source": [
    "run_epochs(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第65535节 她立刻叫“师傅开始工作大腿的城门口的地方转了一个主要市镇里，未会穿迁上唯后，所以老际已经全体或者部\n",
      "final:  第65535节 她立刻叫“师傅开始工作大腿的城门口的地方转了一个主要市镇里，未会穿迁上唯后，所以老际已经全体或者部分书要人谈话已经堆到了办公室的几次关防在近的判决河、城内里却少了。\n",
      "\n",
      "    左亚美了车站要道，我想耽搁没有交代。二个月是个庞代来。”随着列时也失去了自己曾经不再是经济下摆却，每天们面堆满了。\n",
      "\n",
      "    “才员们……”左亚美意识到了。但是查询会，提出今天不是难公室，“晚上，虽然乘客们既然没证据不敢怠慢，面积控制小落――治安法庭的长度车。锦衣卫员出现了空心翼上的厢房。对于一个挂着来协理学院毕业”很少，交通便捷地。\n",
      "\n",
      "    好在这段路在城铁，“成了进口，却说，一旦受刑架上审理案件，要么种得上一个人。”\n",
      "\n",
      "    “她和你提出今天判决犯上参理员一面大量外迁。\n",
      "\n",
      "    如何证明呢？她虽然也整修过警衔乎为一样中在城内的时候在一部分腿脚企划院的城门口还没有煤气灯照英份。\n",
      "\n",
      "    夜间有着灯火的重要机构，她定很便在院子里还有个左亚美一些诸惊世骇俗，商馆。\n",
      "\n",
      "    左亚美出了灯笼就更好，十鞭子两间闪亮很容易事情吗？百仞城内就在女首长那么短寒舌，领周边有个时候已经不够，她定在城里的南宝的事情。她好像东门市――捆在东门市的扩展也不能接发了。百仞城吞免。\n",
      "\n",
      "    “我还没有独立人和做些辅助的地方正说。这里她很少来到了办公室。甚至紧挨，一路灯的李永薰微微鞠到近乎废墟。这衣服非常通亮，偌大的总务九小微弱的时候下，眼睛很得是什么在“和我是室友性命也没有警衔，就是按照英国的裙子堪称惊世骇俗的”李永薰微微鞠了县城里的还不需要这条临时的下力，刑务所看得控制的比较小。\n",
      "\n",
      "    夜间们一起来就睡觉，我们一般的话已经拿到所以乘客了。她想耽密了。她把自己带上相起了车。虽然年龄比自己第一次领到县城里自己带左亚美出示证件命也没有公共马车通亮，现在是晚上专门的手段瞒不是经济一样的事情吗？”\n",
      "\n",
      "    李永薰考虑再，毕竟地仆的裙子，后来看到这衣服，一裹圆住了涂乱院子里很少\n"
     ]
    }
   ],
   "source": [
    "print_example_inline_seed_non_stateful()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1152/6400 [====>.........................] - ETA: 34s - loss: 5.6278"
     ]
    }
   ],
   "source": [
    "run_epochs(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
